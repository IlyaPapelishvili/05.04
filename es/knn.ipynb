{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b518b04cbfe0"
      },
      "source": [
        "##### Copyright 2020 Los autores de TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [

      ],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ca65cda94c8"
      },
      "source": [
        "# Redes neuronales recurrentes (RNN) con Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6873211b02d4"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "Las redes neuronales recurrentes (RNN) son una clase de redes neuronales que es poderosa para modelar datos de secuencia, como series de tiempo o lenguaje natural.\n",
        "\n",
        "De manera esquemática, una capa RNN usa un `for` para iterar sobre los pasos de tiempo de una secuencia, mientras mantiene un estado interno que codifica información sobre los pasos de tiempo que ha visto hasta ahora.\n",
        "\n",
        "La API de Keras RNN está diseñada con un enfoque en:\n",
        "\n",
        "- **Facilidad de uso** : las `keras.layers.RNN` , `keras.layers.LSTM` , `keras.layers.GRU` permiten crear rápidamente modelos recurrentes sin tener que hacer elecciones de configuración difíciles.\n",
        "\n",
        "- **Facilidad de personalización** : también puede definir su propia capa de celda RNN (la parte interna del `for` ) con un comportamiento personalizado y utilizarla con la `keras.layers.RNN` (el `for` sí). Esto le permite crear rápidamente prototipos de diferentes ideas de investigación de una manera flexible con un código mínimo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3600ee25c8e"
      },
      "source": [
        "## Configuración"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71c626bbac35"
      },
      "outputs": [

      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4041a2e9b310"
      },
      "source": [
        "## Capas RNN integradas: un ejemplo sencillo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98e0c38cf95d"
      },
      "source": [
        "Hay tres capas RNN integradas en Keras:\n",
        "\n",
        "1. `keras.layers.SimpleRNN` , un RNN completamente conectado donde la salida del paso de tiempo anterior se alimenta al siguiente paso de tiempo.\n",
        "\n",
        "2. `keras.layers.GRU` , propuesto por primera vez en [Cho et al., 2014](https://arxiv.org/abs/1406.1078) .\n",
        "\n",
        "3. `keras.layers.LSTM` , propuesto por primera vez en [Hochreiter &amp; Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf) .\n",
        "\n",
        "A principios de 2015, Keras tuvo las primeras implementaciones de Python de código abierto reutilizables de LSTM y GRU.\n",
        "\n",
        "Aquí hay un ejemplo simple de un `Sequential` que procesa secuencias de números enteros, incrusta cada número entero en un vector de 64 dimensiones y luego procesa la secuencia de vectores usando una capa `LSTM`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5617759e54e"
      },
      "outputs": [

      ],
      "source": [
        "model = keras.Sequential()\n",
        "# Add an Embedding layer expecting input vocab of size 1000, and\n",
        "# output embedding dimension of size 64.\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "\n",
        "# Add a LSTM layer with 128 internal units.\n",
        "model.add(layers.LSTM(128))\n",
        "\n",
        "# Add a Dense layer with 10 units.\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb8ef33660a0"
      },
      "source": [
        "Los RNN integrados admiten una serie de funciones útiles:\n",
        "\n",
        "- Abandono recurrente, a través de los argumentos `dropout` y `recurrent_dropout`\n",
        "- Capacidad para procesar una secuencia de entrada a la inversa, mediante el argumento `go_backwards`\n",
        "- Desenrollado de bucle (que puede conducir a una gran aceleración al procesar secuencias cortas en la CPU), a través del argumento `unroll`\n",
        "- ...y más.\n",
        "\n",
        "Para obtener más información, consulte la [documentación de la API de RNN](https://keras.io/api/layers/recurrent_layers/) ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43aa4e4f344d"
      },
      "source": [
        "## Salidas y estados\n",
        "\n",
        "De forma predeterminada, la salida de una capa RNN contiene un solo vector por muestra. Este vector es la salida de la celda RNN correspondiente al último paso de tiempo, que contiene información sobre la secuencia de entrada completa. La forma de esta salida es `(batch_size, units)` donde las `units` corresponden al `units` pasado al constructor de la capa.\n",
        "\n",
        "Una capa RNN también puede devolver la secuencia completa de salidas para cada muestra (un vector por paso de tiempo por muestra), si establece `return_sequences=True` . La forma de esta salida es `(batch_size, timesteps, units)` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3294dec91e4"
      },
      "outputs": [

      ],
      "source": [
        "model = keras.Sequential()\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "\n",
        "# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n",
        "model.add(layers.GRU(256, return_sequences=True))\n",
        "\n",
        "# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n",
        "model.add(layers.SimpleRNN(128))\n",
        "\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "266812a04bb2"
      },
      "source": [
        "Además, una capa RNN puede devolver sus estados internos finales. Los estados devueltos se pueden utilizar para reanudar la ejecución de RNN más tarde o [para inicializar otro RNN](https://arxiv.org/abs/1409.3215) . Esta configuración se usa comúnmente en el modelo de secuencia a secuencia de codificador-decodificador, donde el estado final del codificador se usa como el estado inicial del decodificador.\n",
        "\n",
        "Para configurar una capa RNN para que devuelva su estado interno, establezca el parámetro `return_state` `True` al crear la capa. Tenga en cuenta que `LSTM` tiene 2 tensores de estado, pero `GRU` solo tiene uno.\n",
        "\n",
        "Para configurar el estado inicial de la capa, simplemente llame a la capa con el argumento de palabra clave adicional `initial_state` . Tenga en cuenta que la forma del estado debe coincidir con el tamaño de unidad de la capa, como en el ejemplo siguiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ece412e6afbe"
      },
      "outputs": [

      ],
      "source": [
        "encoder_vocab = 1000\n",
        "decoder_vocab = 2000\n",
        "\n",
        "encoder_input = layers.Input(shape=(None,))\n",
        "encoder_embedded = layers.Embedding(input_dim=encoder_vocab, output_dim=64)(\n",
        "    encoder_input\n",
        ")\n",
        "\n",
        "# Return states in addition to output\n",
        "output, state_h, state_c = layers.LSTM(64, return_state=True, name=\"encoder\")(\n",
        "    encoder_embedded\n",
        ")\n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "decoder_input = layers.Input(shape=(None,))\n",
        "decoder_embedded = layers.Embedding(input_dim=decoder_vocab, output_dim=64)(\n",
        "    decoder_input\n",
        ")\n",
        "\n",
        "# Pass the 2 states to a new LSTM layer, as initial state\n",
        "decoder_output = layers.LSTM(64, name=\"decoder\")(\n",
        "    decoder_embedded, initial_state=encoder_state\n",
        ")\n",
        "output = layers.Dense(10)(decoder_output)\n",
        "\n",
        "model = keras.Model([encoder_input, decoder_input], output)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e97a845a372a"
      },
      "source": [
        "## Capas RNN y células RNN\n",
        "\n",
        "Además de las capas RNN integradas, la API RNN también proporciona API a nivel de celda. A diferencia de las capas RNN, que procesan lotes completos de secuencias de entrada, la celda RNN solo procesa un único paso de tiempo.\n",
        "\n",
        "La celda es el interior del `for` de una capa RNN. Envolver una celda dentro de una `keras.layers.RNN` le proporciona una capa capaz de procesar lotes de secuencias, por ejemplo, `RNN(LSTMCell(10))` .\n",
        "\n",
        "Matemáticamente, `RNN(LSTMCell(10))` produce el mismo resultado que `LSTM(10)` . De hecho, la implementación de esta capa en TF v1.x fue simplemente crear la celda RNN correspondiente y envolverla en una capa RNN. Sin embargo, el uso de las `GRU` y `LSTM` integradas permite el uso de CuDNN y es posible que vea un mejor rendimiento.\n",
        "\n",
        "Hay tres celdas RNN integradas, cada una de las cuales corresponde a la capa RNN correspondiente.\n",
        "\n",
        "- `keras.layers.SimpleRNNCell` corresponde a la capa `SimpleRNN`\n",
        "\n",
        "- `keras.layers.GRUCell` corresponde a la capa `GRU`\n",
        "\n",
        "- `keras.layers.LSTMCell` corresponde a la capa `LSTM`\n",
        "\n",
        "La abstracción de celda, junto con la `keras.layers.RNN` , facilitan la implementación de arquitecturas RNN personalizadas para su investigación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60b3b721d500"
      },
      "source": [
        "## Statefulness entre lotes\n",
        "\n",
        "Al procesar secuencias muy largas (posiblemente infinitas), es posible que desee utilizar el patrón de **estado de lotes cruzados** .\n",
        "\n",
        "Normalmente, el estado interno de una capa RNN se restablece cada vez que ve un nuevo lote (es decir, se supone que cada muestra vista por la capa es independiente del pasado). La capa solo mantendrá un estado mientras procesa una muestra determinada.\n",
        "\n",
        "Sin embargo, si tiene secuencias muy largas, es útil dividirlas en secuencias más cortas y alimentar estas secuencias más cortas secuencialmente en una capa RNN sin restablecer el estado de la capa. De esa manera, la capa puede retener información sobre la totalidad de la secuencia, aunque solo vea una subsecuencia a la vez.\n",
        "\n",
        "Puede hacer esto estableciendo `stateful=True` en el constructor.\n",
        "\n",
        "Si tiene una secuencia `s = [t0, t1, ... t1546, t1547]` , la dividiría en, por ejemplo,\n",
        "\n",
        "```\n",
        "s1 = [t0, t1, ... t100]\n",
        "s2 = [t101, ... t201]\n",
        "...\n",
        "s16 = [t1501, ... t1547]\n",
        "```\n",
        "\n",
        "Entonces lo procesarías a través de:\n",
        "\n",
        "```python\n",
        "lstm_layer = layers.LSTM(64, stateful=True)\n",
        "for s in sub_sequences:\n",
        "  output = lstm_layer(s)\n",
        "```\n",
        "\n",
        "Cuando desee borrar el estado, puede usar `layer.reset_states()` .\n",
        "\n",
        "> Nota: En esta configuración, `i` en un lote dado es la continuación de la muestra `i` en el lote anterior. Esto significa que todos los lotes deben contener el mismo número de muestras (tamaño de lote). Por ejemplo, si un lote contiene `[sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100]` , el siguiente lote debe contener `[sequence_A_from_t101_to_t200, sequence_B_from_t101_to_t200]` .\n",
        "\n",
        "Aquí tienes un ejemplo completo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19e72be49a42"
      },
      "outputs": [

      ],
      "source": [
        "paragraph1 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "paragraph2 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "\n",
        "lstm_layer = layers.LSTM(64, stateful=True)\n",
        "output = lstm_layer(paragraph1)\n",
        "output = lstm_layer(paragraph2)\n",
        "output = lstm_layer(paragraph3)\n",
        "\n",
        "# reset_states() will reset the cached state to the original initial_state.\n",
        "# If no initial_state was provided, zero-states will be used by default.\n",
        "lstm_layer.reset_states()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec7c316b19a1"
      },
      "source": [
        "### Reutilización de estado RNN\n",
        "\n",
        "<a id=\"rnn_state_reuse\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cb7a8ac464a"
      },
      "source": [
        "Los estados registrados de la capa RNN no se incluyen en `layer.weights()` . Si desea reutilizar el estado de una capa RNN, puede recuperar el valor de los estados por `layer.states` y usarlo como el estado inicial para una nueva capa a través de la API funcional de Keras como `new_layer(inputs, initial_state=layer.states)` o subclases de modelos.\n",
        "\n",
        "Tenga en cuenta también que es posible que el modelo secuencial no se use en este caso, ya que solo admite capas con una sola entrada y salida, la entrada adicional del estado inicial hace que sea imposible de usar aquí."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "009c5b393adf"
      },
      "outputs": [

      ],
      "source": [
        "paragraph1 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "paragraph2 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "\n",
        "lstm_layer = layers.LSTM(64, stateful=True)\n",
        "output = lstm_layer(paragraph1)\n",
        "output = lstm_layer(paragraph2)\n",
        "\n",
        "existing_state = lstm_layer.states\n",
        "\n",
        "new_lstm_layer = layers.LSTM(64)\n",
        "new_output = new_lstm_layer(paragraph3, initial_state=existing_state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66c1d7f1ccba"
      },
      "source": [
        "## RNN bidireccionales\n",
        "\n",
        "Para secuencias que no sean series de tiempo (por ejemplo, texto), a menudo ocurre que un modelo RNN puede funcionar mejor si no solo procesa la secuencia de principio a fin, sino también hacia atrás. Por ejemplo, para predecir la siguiente palabra en una oración, a menudo es útil tener el contexto alrededor de la palabra, no solo las palabras que vienen antes.\n",
        "\n",
        "Keras proporciona una API fácil para que usted `keras.layers.Bidirectional` tales RNN bidireccionales: la envoltura keras.layers.Bidirectional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cea1781a0c2"
      },
      "outputs": [

      ],
      "source": [
        "model = keras.Sequential()\n",
        "\n",
        "model.add(\n",
        "    layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(5, 10))\n",
        ")\n",
        "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dab57c97a566"
      },
      "source": [
        "Debajo del capó, `Bidirectional` copiará la capa RNN pasada y `go_backwards` campo go_backwards de la capa recién copiada, de modo que procesará las entradas en orden inverso.\n",
        "\n",
        "La salida del `Bidirectional` será, por defecto, la concatenación de la salida de la capa hacia adelante y la salida de la capa hacia atrás. Si necesita un comportamiento de fusión diferente, por ejemplo, concatenación, cambie el `merge_mode` en el constructor contenedor `Bidirectional` Para obtener más detalles sobre `Bidirectional` , consulte [los documentos de la API](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional/) ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18a254dfaa73"
      },
      "source": [
        "## Optimización del rendimiento y kernels CuDNN\n",
        "\n",
        "En TensorFlow 2.0, las capas integradas de LSTM y GRU se actualizaron para aprovechar los kernels CuDNN de forma predeterminada cuando hay una GPU disponible. Con este cambio, las `keras.layers.CuDNNLSTM/CuDNNGRU` han quedado obsoletas y puede crear su modelo sin preocuparse por el hardware en el que se ejecutará.\n",
        "\n",
        "Dado que el kernel CuDNN se construye con ciertas suposiciones, esto significa que la capa **no podrá usar el kernel CuDNN si cambia los valores predeterminados de las capas integradas LSTM o GRU** . P.ej:\n",
        "\n",
        "- Cambiar la función de `activation` `tanh` a otra cosa.\n",
        "- Cambiar la función `recurrent_activation` `sigmoid` a otra cosa.\n",
        "- Usando `recurrent_dropout` &gt; 0.\n",
        "- Establecer `unroll` en Verdadero, lo que obliga a LSTM / GRU a descomponer el `tf.while_loop` interno en un bucle `for`\n",
        "- Estableciendo `use_bias` en False.\n",
        "- Usar enmascaramiento cuando los datos de entrada no están estrictamente rellenados a la derecha (si la máscara corresponde a datos estrictamente rellenados a la derecha, CuDNN aún se puede usar. Este es el caso más común).\n",
        "\n",
        "Para obtener una lista detallada de restricciones, consulte la documentación de las capas [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM/) y [GRU.](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb8de09c4343"
      },
      "source": [
        "### Usar núcleos CuDNN cuando estén disponibles\n",
        "\n",
        "Construyamos un modelo LSTM simple para demostrar la diferencia de rendimiento.\n",
        "\n",
        "Usaremos como secuencias de entrada la secuencia de filas de dígitos MNIST (tratando cada fila de píxeles como un paso de tiempo), y predeciremos la etiqueta del dígito."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e88aab9e73c7"
      },
      "outputs": [

      ],
      "source": [
        "batch_size = 64\n",
        "# Each MNIST image batch is a tensor of shape (batch_size, 28, 28).\n",
        "# Each input sequence will be of size (28, 28) (height is treated like time).\n",
        "input_dim = 28\n",
        "\n",
        "units = 64\n",
        "output_size = 10  # labels are from 0 to 9\n",
        "\n",
        "# Build the RNN model\n",
        "def build_model(allow_cudnn_kernel=True):\n",
        "    # CuDNN is only available at the layer level, and not at the cell level.\n",
        "    # This means `LSTM(units)` will use the CuDNN kernel,\n",
        "    # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.\n",
        "    if allow_cudnn_kernel:\n",
        "        # The LSTM layer with default options uses CuDNN.\n",
        "        lstm_layer = keras.layers.LSTM(units, input_shape=(None, input_dim))\n",
        "    else:\n",
        "        # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n",
        "        lstm_layer = keras.layers.RNN(\n",
        "            keras.layers.LSTMCell(units), input_shape=(None, input_dim)\n",
        "        )\n",
        "    model = keras.models.Sequential(\n",
        "        [\n",
        "            lstm_layer,\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.Dense(output_size),\n",
        "        ]\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcde82cb14d6"
      },
      "source": [
        "Carguemos el conjunto de datos MNIST:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98292f8e71a9"
      },
      "outputs": [

      ],
      "source": [
        "mnist = keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "sample, sample_label = x_train[0], y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "443e5458284f"
      },
      "source": [
        "Creemos una instancia de modelo y entreneémosla.\n",
        "\n",
        "Elegimos `sparse_categorical_crossentropy` como la función de pérdida para el modelo. La salida del modelo tiene la forma `[batch_size, 10]` . El objetivo del modelo es un vector entero, cada uno de los números enteros está en el rango de 0 a 9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f85b57b010e5"
      },
      "outputs": [

      ],
      "source": [
        "model = build_model(allow_cudnn_kernel=True)\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=\"sgd\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99ea5495e375"
      },
      "source": [
        "Ahora, comparemos con un modelo que no usa el kernel CuDNN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4bdff02e617"
      },
      "outputs": [

      ],
      "source": [
        "noncudnn_model = build_model(allow_cudnn_kernel=False)\n",
        "noncudnn_model.set_weights(model.get_weights())\n",
        "noncudnn_model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=\"sgd\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "noncudnn_model.fit(\n",
        "    x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90fc64fbd4ea"
      },
      "source": [
        "Cuando se ejecuta en una máquina con una GPU NVIDIA y CuDNN instalados, el modelo creado con CuDNN es mucho más rápido de entrenar en comparación con el modelo que usa el kernel regular de TensorFlow.\n",
        "\n",
        "El mismo modelo habilitado para CuDNN también se puede utilizar para ejecutar inferencias en un entorno de solo CPU. La `tf.device` continuación solo fuerza la ubicación del dispositivo. El modelo se ejecutará en la CPU de forma predeterminada si no hay GPU disponible.\n",
        "\n",
        "Simplemente ya no tiene que preocuparse por el hardware en el que está ejecutando. ¿No es genial?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e33c62b6029"
      },
      "outputs": [

      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with tf.device(\"CPU:0\"):\n",
        "    cpu_model = build_model(allow_cudnn_kernel=True)\n",
        "    cpu_model.set_weights(model.get_weights())\n",
        "    result = tf.argmax(cpu_model.predict_on_batch(tf.expand_dims(sample, 0)), axis=1)\n",
        "    print(\n",
        "        \"Predicted result is: %s, target result is: %s\" % (result.numpy(), sample_label)\n",
        "    )\n",
        "    plt.imshow(sample, cmap=plt.get_cmap(\"gray\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f940b73a2a6"
      },
      "source": [
        "## RNN con entradas de lista / dictado o entradas anidadas\n",
        "\n",
        "Las estructuras anidadas permiten a los implementadores incluir más información en un solo paso de tiempo. Por ejemplo, un cuadro de video podría tener entrada de audio y video al mismo tiempo. La forma de los datos en este caso podría ser:\n",
        "\n",
        "`[batch, timestep, {\"video\": [height, width, channel], \"audio\": [frequency]}]`\n",
        "\n",
        "En otro ejemplo, los datos de escritura a mano podrían tener las coordenadas xey para la posición actual del lápiz, así como información sobre la presión. Entonces, la representación de datos podría ser:\n",
        "\n",
        "`[batch, timestep, {\"location\": [x, y], \"pressure\": [force]}]`\n",
        "\n",
        "El siguiente código proporciona un ejemplo de cómo construir una celda RNN personalizada que acepta tales entradas estructuradas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f78dc4c1c516"
      },
      "source": [
        "### Definir una celda personalizada que admita entrada / salida anidada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "199faf57f0c5"
      },
      "source": [
        "Consulte [Crear nuevas capas y modelos mediante subclases](https://www.tensorflow.org/guide/keras/custom_layers_and_models/) para obtener detalles sobre cómo escribir sus propias capas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "451cfd5f0cc4"
      },
      "outputs": [

      ],
      "source": [
        "class NestedCell(keras.layers.Layer):\n",
        "    def __init__(self, unit_1, unit_2, unit_3, **kwargs):\n",
        "        self.unit_1 = unit_1\n",
        "        self.unit_2 = unit_2\n",
        "        self.unit_3 = unit_3\n",
        "        self.state_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]\n",
        "        self.output_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]\n",
        "        super(NestedCell, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]\n",
        "        i1 = input_shapes[0][1]\n",
        "        i2 = input_shapes[1][1]\n",
        "        i3 = input_shapes[1][2]\n",
        "\n",
        "        self.kernel_1 = self.add_weight(\n",
        "            shape=(i1, self.unit_1), initializer=\"uniform\", name=\"kernel_1\"\n",
        "        )\n",
        "        self.kernel_2_3 = self.add_weight(\n",
        "            shape=(i2, i3, self.unit_2, self.unit_3),\n",
        "            initializer=\"uniform\",\n",
        "            name=\"kernel_2_3\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        # inputs should be in [(batch, input_1), (batch, input_2, input_3)]\n",
        "        # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]\n",
        "        input_1, input_2 = tf.nest.flatten(inputs)\n",
        "        s1, s2 = states\n",
        "\n",
        "        output_1 = tf.matmul(input_1, self.kernel_1)\n",
        "        output_2_3 = tf.einsum(\"bij,ijkl->bkl\", input_2, self.kernel_2_3)\n",
        "        state_1 = s1 + output_1\n",
        "        state_2_3 = s2 + output_2_3\n",
        "\n",
        "        output = (output_1, output_2_3)\n",
        "        new_states = (state_1, state_2_3)\n",
        "\n",
        "        return output, new_states\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"unit_1\": self.unit_1, \"unit_2\": unit_2, \"unit_3\": self.unit_3}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51355b4089d2"
      },
      "source": [
        "### Construya un modelo RNN con entrada / salida anidada\n",
        "\n",
        "Construyamos un modelo de Keras que use una `keras.layers.RNN` y la celda personalizada que acabamos de definir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2eba7a248eb"
      },
      "outputs": [

      ],
      "source": [
        "unit_1 = 10\n",
        "unit_2 = 20\n",
        "unit_3 = 30\n",
        "\n",
        "i1 = 32\n",
        "i2 = 64\n",
        "i3 = 32\n",
        "batch_size = 64\n",
        "num_batches = 10\n",
        "timestep = 50\n",
        "\n",
        "cell = NestedCell(unit_1, unit_2, unit_3)\n",
        "rnn = keras.layers.RNN(cell)\n",
        "\n",
        "input_1 = keras.Input((None, i1))\n",
        "input_2 = keras.Input((None, i2, i3))\n",
        "\n",
        "outputs = rnn((input_1, input_2))\n",
        "\n",
        "model = keras.models.Model([input_1, input_2], outputs)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "452a99c63b7c"
      },
      "source": [
        "### Entrene el modelo con datos generados aleatoriamente\n",
        "\n",
        "Dado que no hay un buen conjunto de datos candidato para este modelo, utilizamos datos aleatorios de Numpy para la demostración."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3987993cb7be"
      },
      "outputs": [

      ],
      "source": [
        "input_1_data = np.random.random((batch_size * num_batches, timestep, i1))\n",
        "input_2_data = np.random.random((batch_size * num_batches, timestep, i2, i3))\n",
        "target_1_data = np.random.random((batch_size * num_batches, unit_1))\n",
        "target_2_data = np.random.random((batch_size * num_batches, unit_2, unit_3))\n",
        "input_data = [input_1_data, input_2_data]\n",
        "target_data = [target_1_data, target_2_data]\n",
        "\n",
        "model.fit(input_data, target_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b51e87780b0f"
      },
      "source": [
        "Con la `keras.layers.RNN` Keras, solo se espera que defina la lógica matemática para el paso individual dentro de la secuencia, y la `keras.layers.RNN` manejará la iteración de la secuencia por usted. Es una forma increíblemente poderosa de crear rápidamente un prototipo de nuevos tipos de RNN (por ejemplo, una variante de LSTM).\n",
        "\n",
        "Para obtener más detalles, visite los [documentos de](https://https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN/) la API."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [

      ],
      "name": "rnn.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
